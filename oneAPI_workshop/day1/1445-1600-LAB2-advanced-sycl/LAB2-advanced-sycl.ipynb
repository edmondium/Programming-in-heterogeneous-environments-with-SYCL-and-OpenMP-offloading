{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fadd22-60d6-42c7-88a5-38daac8aa77e",
   "metadata": {},
   "source": [
    "# LAB2 advanced SYCL\n",
    "We will be working on VSC5, I will putt only one jobscript for compilation and running of the programs here. It is your job to write the corresponding jobscripts for the exercises.\n",
    "The exercises are described in this NB\n",
    "The first programm 00-Hello.cpp is just for showing you how to compile and run with job scripts and submitting.\n",
    "For all the other exercises you have to write your job and run scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c2fe2-6fb6-4cb7-bdb3-c10f51563cda",
   "metadata": {},
   "source": [
    "# LAB2: get your hands dirty:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a4a1-166d-4cc1-b118-e21eb9faca13",
   "metadata": {},
   "source": [
    "### 00-Hello.cpp \n",
    "is a simple program\n",
    "<br>\n",
    "Compile and run this program using the job scripts and submition commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59f6fe8d-4ade-4b4b-bf08-68b819d07c74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_compile_hello.sh\n"
     ]
    }
   ],
   "source": [
    "%%file job_compile_hello.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name    your_jobname\n",
    "#SBATCH --cluster     vsc5\n",
    "#SBATCH --reservation training_gpu\n",
    "#SBATCH --qos         zen3_0512_a100x2_training\n",
    "#SBATCH --partition   zen3_0512_a100x2\n",
    "#SBATCH --gres        gpu:1\n",
    "#SBATCH --time        00:01:00\n",
    "\n",
    "###########################\n",
    "\n",
    "spack unload\n",
    "spack load cuda@11.8.0\n",
    "\n",
    "source /opt/sw/vsc4/VSC/x86_64/generic/intel/oneapi/setvars.sh \n",
    "export LD_LIBRARY_PATH=/gpfs/opt/sw/cuda-zen/spack-0.19.0/opt/spack/linux-almalinux8-zen/gcc-9.5.0/gcc-12.2.0-nu5le4qn6edhcjiocq7wddclrvdj4xfg/lib64:$LD_LIBRARY_PATH    \n",
    "\n",
    "###########################\n",
    "\n",
    "echo \"**********************************************\"\n",
    "echo \"compiling \"\n",
    "echo \"icpx -fsycl -fsycl-targets=nvptx64-nvidia-cuda,spir64_x86_64 00-Hello.cpp -o 00-Hello.x\"\n",
    "echo \"**********************************************\"\n",
    "icpx -fsycl -fsycl-targets=nvptx64-nvidia-cuda 00-Hello.cpp -o 00-Hello.x\n",
    "echo \"**********************************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff323519-427c-4c5b-9b01-50067ba51127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch: Allocating 50.0 % of cpu resources: 64 / 128.\n",
      "sbatch: Number of tasks adjusted to 64.\n",
      "Submitted batch job 1620842 on cluster vsc5\n"
     ]
    }
   ],
   "source": [
    "!sbatch job_compile_hello.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85df2d0d-51d0-4cf4-9e9c-9d5fd7132fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID            PARTITION     NAME     USER ST       TIME  NODES     NODELIST(REASON)\n",
      "           1620842     zen3_0512_a100x2 your_job trainee9  R       0:05      1            n3072-006\n",
      "           1620734    zen3_0512_jupyter vsc5_jh_ trainee9  R    2:42:59      1            n3503-022\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6d8000-3175-4bf0-ba2b-62324511ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 173\n",
      "-rw-r--r-- 1 trainee96 p70824    481 Apr  7  2023 00-Hello.cpp\n",
      "drwxr-xr-x 2 trainee96 p70824   4096 Dec  8 12:38 pics\n",
      "drwxrwxr-x 2 trainee96 p70824   4096 Dec  8 13:08 usm\n",
      "drwxrwxr-x 2 trainee96 p70824   4096 Dec  8 13:11 sycl\n",
      "-rw-rw-r-- 1 trainee96 p70824    832 Dec  8 13:24 job_cpu_compile_hello.sh\n",
      "-rw-rw-r-- 1 trainee96 p70824    729 Dec  8 13:25 job_cpu_run_hello.sh\n",
      "drwxrwxr-x 2 trainee96 p70824   4096 Dec  8 14:00 buffer\n",
      "-rw-r--r-- 1 trainee96 p70824    729 Dec 11 06:19 job_run_hello.sh\n",
      "-rw-rw-r-- 1 trainee96 p70824  30425 Dec 11 08:36 LAB2-advanced-sycl.ipynb\n",
      "-rw-r--r-- 1 trainee96 p70824    883 Dec 11 08:37 job_compile_hello.sh\n",
      "-rw-r--r-- 1 trainee96 p70824    700 Dec 11 08:37 slurm-1620842.out\n",
      "-rwxr-xr-x 1 trainee96 p70824 136864 Dec 11 08:37 00-Hello.x\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61e5786-fa6e-43b1-93be-1fad7bd18072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ":: initializing oneAPI environment ...\n",
      "   slurm_script: BASH_VERSION = 4.4.20(1)-release\n",
      "   args: Using \"$@\" for setvars.sh arguments: \n",
      ":: advisor -- latest\n",
      ":: ccl -- latest\n",
      ":: compiler -- latest\n",
      ":: dal -- latest\n",
      ":: debugger -- latest\n",
      ":: dev-utilities -- latest\n",
      ":: dnnl -- latest\n",
      ":: dpcpp-ct -- latest\n",
      ":: dpl -- latest\n",
      ":: inspector -- latest\n",
      ":: ipp -- latest\n",
      ":: ippcp -- latest\n",
      ":: itac -- latest\n",
      ":: mkl -- latest\n",
      ":: mpi -- latest\n",
      ":: tbb -- latest\n",
      ":: vtune -- latest\n",
      ":: oneAPI environment initialized ::\n",
      " \n",
      "**********************************************\n",
      "compiling \n",
      "icpx -fsycl -fsycl-targets=nvptx64-nvidia-cuda,spir64_x86_64 00-Hello.cpp -o 00-Hello.x\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-1620842.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7b5fe58-1ecb-4811-9def-ff246329bf94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_run_hello.sh\n"
     ]
    }
   ],
   "source": [
    "%%file job_run_hello.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name    name\n",
    "#SBATCH --cluster     vsc5\n",
    "#SBATCH --reservation training_gpu\n",
    "#SBATCH --qos         zen3_0512_a100x2_training\n",
    "#SBATCH --partition   zen3_0512_a100x2\n",
    "#SBATCH --gres        gpu:1\n",
    "#SBATCH --time        00:01:00\n",
    "###########################\n",
    "\n",
    "spack unload\n",
    "spack load cuda@11.8.0\n",
    "\n",
    "source /opt/sw/vsc4/VSC/x86_64/generic/intel/oneapi/setvars.sh \n",
    "export LD_LIBRARY_PATH=/gpfs/opt/sw/cuda-zen/spack-0.19.0/opt/spack/linux-almalinux8-zen/gcc-9.5.0/gcc-12.2.0-nu5le4qn6edhcjiocq7wddclrvdj4xfg/lib64:$LD_LIBRARY_PATH    \n",
    "\n",
    "###########################\n",
    "\n",
    "echo \"starting program simple-sycl-app on the nvidia gpu\"\n",
    "echo \"**********************************************\"\n",
    "./00-Hello.x\n",
    "echo \"**********************************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daed4b07-dfb6-4bbc-aebf-25a4c3b5fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch: GPU partition selected but GPU count not specified - selecting all GPUs.\n",
      "Submitted batch job 1620843 on cluster vsc5\n"
     ]
    }
   ],
   "source": [
    "!sbatch job_run_hello.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a292ed88-9bbb-4818-961b-88e3a9ad30d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID            PARTITION     NAME     USER ST       TIME  NODES     NODELIST(REASON)\n",
      "           1620734    zen3_0512_jupyter vsc5_jh_ trainee9  R    2:45:58      1            n3503-022\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369562c-272c-4eab-ada8-4cd6e83778c4",
   "metadata": {},
   "source": [
    "### Check the output of your program\n",
    "#### What is the secret msg?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56418a75-2e87-468e-acac-897e9e3fd9d8",
   "metadata": {},
   "source": [
    "## For the additional exercises you need to write your own jobscripts,\n",
    "## the programs are in the directories usm and buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6f402-7b83-41e9-a09c-f66ec58a94f6",
   "metadata": {},
   "source": [
    "# Unified Shared Memory (USM)\n",
    "- What is USM?\n",
    "- Types of USM\n",
    "- Code: Implicit USM\n",
    "- Code: Explicit USM\n",
    "- Data Dependency in USM\n",
    "- Code: Data Dependency in-order queues\n",
    "- Code: Data Dependency out-of-order queues\n",
    "- Lab Exercise: Unified Shared Memory\n",
    "<br>\n",
    "\n",
    "## Learning Objectives\n",
    "- Use new SYCL2020 features such as Unified Shared Memory to simplify programming.\n",
    "- Understand implicit and explicit way of moving memory using USM.\n",
    "- Solve data dependency between kernel tasks in optimal way.\n",
    "<br>\n",
    "\n",
    "## What is Unified Shared Memory?\n",
    "Unified Shared Memory (USM) is a pointer-based memory management in SYCL. USM is a pointer-based approach that\n",
    "should be familiar to C and C++ programmers who use malloc or new to allocate data. USM simplifies development for\n",
    "the programmer when porting existing C/C++ code to SYCL.\n",
    "<br>\n",
    "\n",
    "## Developer view of USM\n",
    "The picture below shows developer view of memory without USM and with USM.\n",
    "<br>\n",
    "With USM, the developer can reference that same memory object in host and device code.\n",
    "![](pics/usm.png)\n",
    "\n",
    "## Types of USM\n",
    "Unified shared memory provides both explicit and implicit models for managing memory.\n",
    "![](pics/usm_2.png)\n",
    "\n",
    "## USM Syntax\n",
    "USM Initialization: The initialization below shows example of shared allocation using malloc_shared, the \"q\" queue\n",
    "parameter provides information about the device that memory is accessible.\n",
    "<br>\n",
    "```c++\n",
    "int *data = malloc_shared<int>(N, q);\n",
    "```\n",
    "\n",
    "OR you can use familiar C++/C style malloc:\n",
    "\n",
    "```c++\n",
    "int *data = static_cast<int *>(malloc_shared(N * sizeof(int), q));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95ad71-e9c8-44bb-b3bd-e8a53f4498ae",
   "metadata": {},
   "source": [
    "## Freeing USM:\n",
    "```c++\n",
    "free(data, q);\n",
    "```\n",
    "\n",
    "## USM Implicit Data Movement\n",
    "The SYCL code below shows an implementation of USM using malloc_shared, in which data movement happens implicitly\n",
    "between host and device. Useful to get functional quickly with minimum amount of code and developers will not\n",
    "having worry about moving memory between host and device.\n",
    "The SYCL code below demonstrates USM Implicit Data Movement:\n",
    "\n",
    "```c++\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "static const int N = 16;\n",
    "\n",
    "int main()\n",
    "{\n",
    "    queue q;\n",
    "    std::cout << \"Device : \" << q.get_device().get_info<info::device::name>() << \"\\n\";\n",
    "    //# USM allocation using malloc_shared\n",
    "    int *data = malloc_shared<int>(N, q);\n",
    "    //# Initialize data array\n",
    "    for (int i = 0; i < N; i++) data[i] = i;\n",
    "        \n",
    "    //# Modify data array on device\n",
    "    q.parallel_for(range<1>(N), [=](id<1> i) { data[i] *= 2; }).wait();\n",
    "\n",
    "    //# print output\n",
    "    for (int i = 0; i < N; i++) std::cout << data[i] << \"\\n\";\n",
    "        \n",
    "    free(data, q);\n",
    "    return 0;\n",
    "        \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daa43b-d9e9-4ad6-bb23-824feb2031ca",
   "metadata": {},
   "source": [
    "# Exercises USM\n",
    "## usm1\n",
    "1. Inspect the code in usm/usm.cpp file.\n",
    "2. Compile and run this code using our jobscripts for offloading to our nvidia GPUs.\n",
    "\n",
    "## USM Explicit Data Movement\n",
    "The SYCL code below shows an implementation of USM using malloc_device, in which data movement between host and\n",
    "device should be done explicitly by developer using memcpy. This allows developers to have more controlled\n",
    "movement of data between host and device.\n",
    "<br>\n",
    "The SYCL code below demonstrates USM Explicit Data Movement:\n",
    "\n",
    "```c++\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "static const int N = 16;\n",
    "\n",
    "int main()\n",
    "{\n",
    "    queue q;\n",
    "    std::cout << \"Device : \" << q.get_device().get_info<info::device::name>() << \"\\n\";\n",
    "\n",
    "    //# initialize data on host\n",
    "    int *data = static_cast<int *>(malloc(N * sizeof(int)));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) data[i] = i;\n",
    "\n",
    "    //# Explicit USM allocation using malloc_device\n",
    "    int *data_device = malloc_device<int>(N, q);\n",
    "\n",
    "    //# copy mem from host to device\n",
    "    q.memcpy(data_device, data, sizeof(int) * N).wait();\n",
    "    \n",
    "    //# update device memory\n",
    "    q.parallel_for(range<1>(N), [=](id<1> i) { data_device[i] *= 2; }).wait();\n",
    "    \n",
    "    //# copy mem from device to host\n",
    "    q.memcpy(data, data_device, sizeof(int) * N).wait();\n",
    "    \n",
    "    //# print output\n",
    "    for (int i = 0; i < N; i++) std::cout << data[i] << \"\\n\";\n",
    "    \n",
    "    free(data_device, q);\n",
    "    free(data);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32116fa5-2d26-4f86-b457-e91c329b5dc8",
   "metadata": {},
   "source": [
    "# Exercises USM\n",
    "## usm2\n",
    "1. Inspect the code in usm/usm_explicit.cpp file\n",
    "2. Compile and run this code\n",
    "3. What happens if you don’t wait on the event returned from parallel_for on line 26?\n",
    "<br>\n",
    "\n",
    "## When to use USM?\n",
    "SYCL* Buffers are powerful and elegant. Use them if the abstraction applies cleanly in your application, and/or if\n",
    "buffers aren’t disruptive to your development. However, replacing all pointers and arrays with buffers in a C++ program\n",
    "can be a burden to programmers so in this case consider using USM\n",
    "\n",
    "## USM provides a familiar pointer-based C++ interface:\n",
    "- Useful when porting C++ code to SYCL by minimizing changes\n",
    "- Use shared allocations when porting code to get functional quickly. Note that shared allocation is not intended to provide peak performance out of box.\n",
    "- Use explicit USM allocations when controlled data movement is needed.\n",
    "\n",
    "## Data dependency in USM\n",
    "When using unified shared memory, dependences between tasks must be specified using events since tasks execute\n",
    "asynchronously and multiple tasks can execute simultaneously.\n",
    "<br>\n",
    "Programmers may either explicitly wait on event objects or use the depends_on method inside a command group to\n",
    "specify a list of events that must complete before a task may begin.\n",
    "<br>\n",
    "In the example below, the two kernel tasks are updating the same data array, these two kernels can execute\n",
    "simultaneously and may cause undesired result. The first task must be complete before the second can begin, the next\n",
    "section will show different ways the data dependency can be resolved.\n",
    "\n",
    "```c++\n",
    "q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });\n",
    "\n",
    "q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 3; });\n",
    "```\n",
    "\n",
    "## Different options to manage data dependency when using USM:\n",
    "- wait() on kernel task\n",
    "- use in_order queue property\n",
    "- use depends_on method\n",
    "\n",
    "### wait()\n",
    "- Use q.wait() on kernel task to wait before the next dependent task can begin, however it will block execution on host.\n",
    "\n",
    "```c++\n",
    "q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });\n",
    "q.wait(); // <--- wait() will make sure that task is complete before continuing\n",
    "q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 3; });\n",
    "```\n",
    "\n",
    "### in_order queue property\n",
    "- Use in_order queue property for the queue, this will serialize all the kerenel tasks. Note that execution will not overlap even if the queues have no data dependency.\n",
    "\n",
    "```c++\n",
    "queue q{property::queue::in_order()}; // <--- this will serialize all kernel tasks\n",
    "q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });\n",
    "q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 3; });\n",
    "```\n",
    "\n",
    "### depends_on\n",
    "- Use h.depends_on(e) method in command group to specify events that must complete before a task may begin.\n",
    "\n",
    "```c++\n",
    "auto e = q.submit([&](handler &h)\n",
    "{ // <--- e is event for kernel task\n",
    "    h.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });\n",
    "});\n",
    "\n",
    "q.submit([&](handler &h)\n",
    "{\n",
    "    h.depends_on(e); // <--- waits until event e is complete\n",
    "    h.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 3; });\n",
    "});\n",
    "```\n",
    "\n",
    "- You can also use a simplified way of specifying dependencies by passing an extra parameter in parallel_for\n",
    "\n",
    "```c++\n",
    "auto e = q.parallel_for(range<1>(N), [=](id<1> i) { data[i] += 2; });\n",
    "\n",
    "q.parallel_for(range<1>(N), e, [=](id<1> i) { data[i] += 3; });\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a2191-249f-496c-ac4b-947f0af7f3e1",
   "metadata": {},
   "source": [
    "# Exercises USM\n",
    "## usm3\n",
    "Code Example: USM and Data dependency 1\n",
    "<br>\n",
    "The code in usm/usm_data.cpp uses USM and has three kernels that are submitted to the device. Each kernel modifies\n",
    "the same data array. There is data dependency between the three queue submissions, so the code needs to be fixed to\n",
    "get desired output of 20.\n",
    "\n",
    "1. Inspect the code in usm/usm_data.cpp file and fix the bug.\n",
    "<br>\n",
    "There are three solutions: use in_order queue property or use wait() event or use depends_on() method.\n",
    "<br>\n",
    "### HINT\n",
    "- Add wait() for each queue submit\n",
    "- Implement depends_on() method in second and third kernel task\n",
    "- Use in_order queue property instead of regular queue: queue q{property::queue::in_order()};\n",
    "\n",
    "2. Compile and run this code using job scripts on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c27cb-fa3b-46b7-8a7d-1754bc40986b",
   "metadata": {},
   "source": [
    "# Exercises USM\n",
    "## usm4\n",
    "Code Example: USM and Data dependency 2\n",
    "<br>\n",
    "The code in lab2/usm_data2.cpp uses USM and has three kernels that are submitted to device. The first two kernels\n",
    "modify two different memory objects and the third one has a dependency on the first two. There is data dependency\n",
    "between the three queue submissions, so the code needs to be fixed to get the desired output of 25.\n",
    "\n",
    "1. Inspect the code in usm/usm_data2.cpp file and implement the solution.\n",
    "<br>\n",
    "- Implementing depends_on() method gets the best performance\n",
    "- Using in_order queue property or wait() will get results but not the most efficient\n",
    "\n",
    "### HINT:\n",
    "```c++\n",
    "auto e1 = ...   ;\n",
    "auto e2 = ...   ;\n",
    "    \n",
    "q.parallel_for(range<1>(N), {e1, e2}, [=](id<1> i)\n",
    "{\n",
    "    ...\n",
    "});\n",
    "```\n",
    "\n",
    "2. Compile and run this code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143575b-ab3e-4540-9784-d61b43dc2fdd",
   "metadata": {},
   "source": [
    "# Exercises USM\n",
    "## usm5\n",
    "Complete the coding exercise using Unified Shared Memory concepts.\n",
    "<br>\n",
    "1. Complete the code in usm/usm_lab.cpp file by writing the missing code (look for comments)\n",
    "<br>\n",
    "- The code has two arrays data1 and data2 initialized on host\n",
    "- Create USM device allocation for data1 and data2 and copy data to device\n",
    "- Create two kernel tasks, one to update data1 with sqrt of values and another to update data2 with sqrt of values\n",
    "- Create a third kernel task to add data2 into data1\n",
    "- Copy data1 back to host and verify results\n",
    "<br>\n",
    "2. Compile and run this code as usual\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b30ba-59b0-49a8-870c-0d2f7e2921d2",
   "metadata": {},
   "source": [
    "# Buffer Memory Model\n",
    "Buffers encapsulate data in a SYCL application across both devices and host. Accessors is the mechanism to access\n",
    "buffer data.\n",
    "As explained earlier in USM model section, offloading computation requires copying data between host and\n",
    "device. In Buffer Memory model SYCL does not require the programmer to manage the data copies. By\n",
    "creating Buffers and Accessors, SYCL ensures that the data is available to host and device without any programmer\n",
    "effort. SYCL also allows the programmer explicit control over data movement when it is necessary to achieve best\n",
    "performance. The code below shows Simple Vector addition using SYCL and Buffers. Read through the comments\n",
    "addressed in step 1 through step 6.\n",
    "<br>\n",
    "Buffers encapsulate data in a SYCL application across both devices and host. Accessors is the mechanism to access\n",
    "buffer data.\n",
    "<br>\n",
    "As explained earlier in USM model section, offloading computation requires copying data between host and\n",
    "device. In Buffer Memory model SYCL does not require the programmer to manage the data copies. By\n",
    "creating Buffers and Accessors, SYCL ensures that the data is available to host and device without any programmer\n",
    "effort. SYCL also allows the programmer explicit control over data movement when it is necessary to achieve best\n",
    "performance. The code below shows Simple Vector addition using SYCL and Buffers. Read through the comments\n",
    "addressed in step 1 through step 6.\n",
    "\n",
    "```c++\n",
    "void SYCL_code(int* a, int* b, int* c, int N)\n",
    "{\n",
    "    //Step 1: create a device queue\n",
    "    //(developer can specify a device type via device selector or use default selector)\n",
    "    queue q;\n",
    "\n",
    "    //Step 2: create buffers (represent both host and device memory)\n",
    "    buffer buf_a(a, range<1>(N));\n",
    "    buffer buf_b(b, range<1>(N));\n",
    "    buffer buf_c(c, range<1>(N));\n",
    "\n",
    "    //Step 3: submit a command for (asynchronous) execution\n",
    "    q.submit([&](handler &h)\n",
    "    {\n",
    "\n",
    "        //Step 4: create buffer accessors to access buffer data on the device\n",
    "        accessor A(buf_a,h,read_only);\n",
    "        accessor B(buf_b,h,read_only);\n",
    "        accessor C(buf_c,h,write_only);\n",
    "\n",
    "        //Step 5: send a kernel (lambda) for execution\n",
    "        h.parallel_for(N, [=](auto i)\n",
    "        {\n",
    "            //Step 6: write a kernel\n",
    "            //Kernel invocations are executed in parallel\n",
    "            //Kernel is invoked for each element of the range\n",
    "            //Kernel invocation has access to the invocation id\n",
    "            C[i] = A[i] + B[i];\n",
    "        });\n",
    "    });\n",
    "}\n",
    "```\n",
    "\n",
    "## Vector Add implementation using USM and Buffers\n",
    "The SYCL code below shows vector add computation implemented using USM and Buffers memory model:\n",
    "\n",
    "```c++\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "// kernel function to compute vector add using Unified Shared memory model (USM)\n",
    "void kernel_usm(int* a, int* b, int* c, int N)\n",
    "{\n",
    "    //Step 1: create a device queue\n",
    "    queue q;\n",
    "\n",
    "    //Step 2: create USM device allocation\n",
    "    auto a_device = malloc_device<int>(N, q);\n",
    "    auto b_device = malloc_device<int>(N, q);\n",
    "    auto c_device = malloc_device<int>(N, q);\n",
    "\n",
    "    //Step 3: copy memory from host to device\n",
    "    q.memcpy(a_device, a, N*sizeof(int));\n",
    "    q.memcpy(b_device, b, N*sizeof(int));\n",
    "    q.wait();\n",
    "\n",
    "    //Step 4: send a kernel (lambda) for execution\n",
    "    q.parallel_for(N, [=](auto i)\n",
    "    {\n",
    "        //Step 5: write a kernel\n",
    "        c_device[i] = a_device[i] + b_device[i];\n",
    "    }).wait();\n",
    "    \n",
    "    //Step 6: copy the result back to host\n",
    "    q.memcpy(c, c_device, N*sizeof(int)).wait();\n",
    "\n",
    "    //Step 7: free device allocation\n",
    "    free(a_device, q);\n",
    "    free(b_device, q);\n",
    "    free(c_device, q);\n",
    "}\n",
    "\n",
    "// kernel function to compute vector add using Buffer memory model\n",
    "void kernel_buffers(int* a, int* b, int* c, int N)\n",
    "{\n",
    "    //Step 1: create a device queue\n",
    "    queue q;\n",
    "\n",
    "    //Step 2: create buffers\n",
    "    buffer buf_a(a, range<1>(N));\n",
    "    buffer buf_b(b, range<1>(N));\n",
    "    buffer buf_c(c, range<1>(N));\n",
    "\n",
    "    //Step 3: submit a command for (asynchronous) execution\n",
    "    q.submit([&](handler &h)\n",
    "    {\n",
    "        //Step 4: create buffer accessors to access buffer data on the device\n",
    "        accessor A(buf_a, h, read_only);\n",
    "        accessor B(buf_b, h, read_only);\n",
    "        accessor C(buf_c, h, write_only);\n",
    "        //Step 5: send a kernel (lambda) for execution\n",
    "        h.parallel_for(N, [=](auto i)\n",
    "        {\n",
    "            //Step 6: write a kernel\n",
    "            C[i] = A[i] + B[i];\n",
    "        });\n",
    "    });\n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "    // initialize data arrays on host\n",
    "    constexpr int N = 256;\n",
    "    int a[N], b[N], c[N];\n",
    "    for (int i=0; i<N; i++)\n",
    "    {\n",
    "        a[i] = 1;\n",
    "        b[i] = 2;\n",
    "    }\n",
    "    \n",
    "    // initialize c = 0 and offload computation using USM, print output\n",
    "    for (int i=0; i<N; i++) c[i] = 0;\n",
    "\n",
    "    kernel_usm(a, b, c, N);\n",
    "    std::cout << \"Vector Add Output (USM): \\n\";\n",
    "    for (int i=0; i<N; i++)std::cout << c[i] << \" \";std::cout << \"\\n\";\n",
    "\n",
    "    // initialize c = 0 and offload computation using USM, print output\n",
    "    for (int i=0; i<N; i++) c[i] = 0;\n",
    "    std::cout << \"Vector Add Output (Buffers): \\n\";\n",
    "    kernel_buffers(a, b, c, N);\n",
    "    \n",
    "    for (int i=0; i<N; i++)std::cout << c[i] << \" \";std::cout << \"\\n\";\n",
    "}\n",
    "```\n",
    "\n",
    "###  Exercise Buffer1\n",
    "1. Inspect the code in buffer/vector_add_usm_buffers.cpp file showing vector add computation implemented using USM and Buffers memory model.\n",
    "2. Compile and run this code using jobscripts\n",
    "3. Set the environment variable SYCL_PI_TRACE to enable the tracing of plugins/devices discovery. You should be able to check on which device you are running:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb530e34-dc16-4dbb-a754-8004642cccc3",
   "metadata": {},
   "source": [
    "## Exercise Buffer 2\n",
    "\n",
    "1. Try to solve the exercise described in the cpp file buffer/buffer.cpp.\n",
    "\n",
    "```c++\n",
    "#include <CL/sycl.hpp>\n",
    "using namespace sycl;\n",
    "constexpr int N = 42;\n",
    "\n",
    "// Buffers may not be directly accessed by the host and device (except\n",
    "// through advanced and infrequently used mechanisms not described here).\n",
    "// Instead, we must create accessors in order to read and write to buffers.\n",
    "// Accessors provide the runtime with information about how we plan to use\n",
    "// the data in buffers, allowing it to correctly schedule data movement.\n",
    "\n",
    "// In this exercise the hostVector is declarated and initialized\n",
    "// on the host.\n",
    "\n",
    "// In the device block a buffer buf is created to interact with\n",
    "// hostVector on the host.\n",
    "\n",
    "//-----------------TASK-----------------------------\n",
    "// Create buffer and an accessor to update the\n",
    "// buffer buf on the device\n",
    "// and write a h.parallel_for in which the indices\n",
    "// are written to a and thereby also to buf\n",
    "//-----------------TASK-----------------------------\n",
    "\n",
    "int main()\n",
    "{\n",
    "    std::vector<int> hostVector(N, 3);\n",
    "\n",
    "    std::cout << \"printing hostVector before computation \\n\" ;\n",
    "    for (int i = 0; i < N; i++) std::cout << hostVector[i] << \" \";\n",
    "    std::cout << \"\\n\" ;\n",
    "\n",
    "    {\n",
    "        queue q;\n",
    "        // Create a buffer buf for the hostVector\n",
    "        // YOUR CODE GOES HERE\n",
    "        \n",
    "\t\n",
    "\tq.submit( [&] (handler& h)\n",
    "         {\n",
    "        // Create an accessor a for buf\n",
    "        // and write a h.parallel_for in which the indices\n",
    "        // are written to a and thereby also to buf\n",
    "        // YOUR CODE GOES HERE\n",
    "         \n",
    "\t \n",
    "\t } );\n",
    "\n",
    "    }\n",
    "\n",
    "    // When exiting the scope of the devices destroys the buffer\n",
    "    // and updating the hostVector\n",
    "\n",
    "    std::cout << \"printing hostVector after computation \\n\" ;\n",
    "    for (int i = 0; i < N; i++) std::cout << hostVector[i] << \" \";\n",
    "    std::cout << \"\\n\" ;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "2. Build and test the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665883a7-e82f-4168-952d-618c4b292920",
   "metadata": {},
   "source": [
    "## Exercise Buffer and Accessor \n",
    "1. Try to solve the exercise described in the cpp file buffer/buffer_accessor.cpp\n",
    "\n",
    "```c++\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "int main() {\n",
    "  const int N = 16;\n",
    "\n",
    "  //# Initialize a vector and print values\n",
    "  std::vector<int> v1(N, 11);\n",
    "  std::vector<int> v2(N, 22);\n",
    "  std::vector<int> v3(N, 0);\n",
    "\n",
    "  std::cout<<\"\\nInput V1: \";\n",
    "  for (int i = 0; i < N; i++) std::cout << v1[i] << \" \";\n",
    "  std::cout<<\"\\nInput V2: \";\n",
    "  for (int i = 0; i < N; i++) std::cout << v2[i] << \" \";\n",
    "  std::cout<<\"\\nInput V3: \";\n",
    "  for (int i = 0; i < N; i++) std::cout << v3[i] << \" \";\n",
    "\n",
    "  {\n",
    "\n",
    "    //# STEP 1 : Create buffers for the three vectors\n",
    "\n",
    "    //# YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    //# Submit task to add vector\n",
    "    queue q;\n",
    "    q.submit([&](handler &h) {\n",
    "\n",
    "      //# STEP 2 - create accessors for buffers with access permissions\n",
    "\n",
    "      //# YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "\n",
    "      h.parallel_for(range<1>(N), [=](id<1> i) {\n",
    "\n",
    "        //# STEP 3 : Implement kernel code to add v3 = v1 + v2\n",
    "\n",
    "        //# YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      });\n",
    "    });\n",
    "\n",
    "  }\n",
    "\n",
    "  //# Print Output values\n",
    "  std::cout<<\"\\nOutput V3: \";\n",
    "  for (int i = 0; i < N; i++) std::cout<< v3[i] << \" \";\n",
    "  std::cout<<\"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
